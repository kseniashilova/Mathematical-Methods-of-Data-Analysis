{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 4\n",
    "\n",
    "### Attention!\n",
    "* For tasks where <ins>text answer</ins> is required **Russian language** is **allowed**.\n",
    "* If a task asks you to describe something (make coclusions) then **text answer** is **mandatory** and **is** part of the task\n",
    "* **Do not** upload the dataset to the grading system (we already have it)\n",
    "* We **only** accept **ipynb** notebooks. If you use Google Colab then you'll have to download the notebook before passing the homework\n",
    "* **Do not** use python loops instead of NumPy vector operations over NumPy vectors - it significantly decreases performance (see why https://blog.paperspace.com/numpy-optimization-vectorization-and-broadcasting/), will be punished with -0.25 for **every** task. \n",
    "\n",
    "\n",
    "### Contents\n",
    "\n",
    "#### Decision Trees - 7 points\n",
    "* [Task 1](#task1) (0.5 points)\n",
    "* [Task 2](#task2) (0.5 points)\n",
    "* [Task 3](#task3) (2 points)\n",
    "* [Task 4](#task4) (0.5 points)\n",
    "* [Task 5](#task5) (0.5 points)\n",
    "* [Task 6](#task6) (2 points)\n",
    "* [Task 7](#task7) (0.5 points)\n",
    "* [Task 8](#task8) (0.5 points)\n",
    "\n",
    "#### Ensembles - 3 points\n",
    "* [Task 1](#task2_1) (1 point)\n",
    "* [Task 2](#task2_2) (0.7 points)\n",
    "* [Task 3](#task2_3) (0.5 points)\n",
    "* [Task 4](#task2_4) (0.7 points)\n",
    "* [Task 5](#task2_5) (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (11, 5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will be implementing decision tree for the regression by hands. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 <a id=\"task1\"></a> (0.5 points)\n",
    "\n",
    "Implement the function `H()` which calculates impurity criterion. We will be training regression tree, therefore, impurity criterion will be variance.\n",
    "\n",
    "* You cannot use loops\n",
    "* If `y` is empty, the function should return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(y):\n",
    "    \"\"\"\n",
    "    Calculate impurity criterion\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        array of objects target values in the node\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    H(R) : float\n",
    "        Impurity in the node (measuread by variance)\n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    #чтобы посчитать дисперсию, нужно взять мат ожидание от квадрата разности значений и мат ожидания. То есть\n",
    "    #V[y] = E[(y-E(y))^2]\n",
    "    E_y = np.mean(y)\n",
    "    return np.mean(np.power(y-E_y, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "assert np.allclose(H(np.array([4,2,2, 2])), 0.75)\n",
    "assert np.allclose(H(np.array([])), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 <a id=\"task2\"></a>  (0.5 points)\n",
    "\n",
    "To find the best split in the node we need to calculate the cost function. Denote: \n",
    "- `R` all the object in the node\n",
    "- `j` index of the feature selected for the split\n",
    "- `t` threshold\n",
    "- `R_l` and `R_r` objects in the left and right child nodes correspondingly\n",
    "\n",
    "We get the following cost function:\n",
    "\n",
    "$$\n",
    "Q(R, j, t) =\\frac{|R_\\ell|}{|R|}H(R_\\ell) + \\frac{|R_r|}{|R|}H(R_r) \\to \\min_{j, t},\n",
    "$$\n",
    "\n",
    "Implement the function `Q`, which should calculate value of the cost function for a given feature and threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q(X, y, j, t):\n",
    "    \"\"\"\n",
    "    Calculate cost function\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        array of objects in the node \n",
    "    y : ndarray\n",
    "        array of target values in the node \n",
    "    j : int\n",
    "        feature index (column in X)\n",
    "    t : float\n",
    "        threshold\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : float\n",
    "        Value of the cost function\n",
    "    \"\"\"   \n",
    "    #До порога и после (слева и справа)\n",
    "    R_l = y[X[:,j] <= t]\n",
    "    R_r = y[X[:,j] > t]\n",
    "    #Считаем взвешенную сумму по формуле выше\n",
    "    Q = len(R_l)*H(R_l)/len(X) + len(R_r)*H(R_r)/len(X)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 <a id=\"task3\"></a>  (2 points)\n",
    "\n",
    "Now, let's implement `MyDecisionTreeRegressor` class. More specifically, you need to implement the following methods:\n",
    "\n",
    "- `best_split`\n",
    "- `grow_tree`\n",
    "- `get_prediction`\n",
    "\n",
    "Read docstrings for more details. Do not forget to use function `Q` implemented above, when finding the `best_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Class for a decision tree node.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    right : Node() or None\n",
    "        Right child\n",
    "    right : Node() or None\n",
    "        Left child\n",
    "    threshold: float\n",
    "        \n",
    "    column: int\n",
    "        \n",
    "    depth: int\n",
    "        \n",
    "    prediction: float\n",
    "        prediction of the target value in the node (average values calculated on a train dataset)\n",
    "    is_terminal:bool\n",
    "        indicates whether it is a terminal node (leaf) or not\n",
    "    \"\"\"    \n",
    "    def __init__(self):        \n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        self.threshold = None\n",
    "        self.column = None\n",
    "        self.depth = None\n",
    "        self.is_terminal = False\n",
    "        self.prediction = None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        if self.is_terminal:\n",
    "            node_desc = 'Pred: {:.2f}'.format(self.prediction)\n",
    "        else:\n",
    "            node_desc = 'Col {}, t {:.2f}, Pred: {:.2f}'.format(self.column, self.threshold, self.prediction)\n",
    "        return node_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class MyDecisionTreeRegressor(RegressorMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Class for a Decision Tree Regressor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_depth : int\n",
    "        Max depth of a decision tree.\n",
    "    min_samples_split : int\n",
    "        Minimal number of samples (objects) in a node to make a split.\n",
    "    \"\"\" \n",
    "    def __init__(self, max_depth=3, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "    \n",
    "    def _more_tags(self):\n",
    "        return {\n",
    "            'requires_y': False\n",
    "        }\n",
    "            \n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best split in terms of Q of data in a given decision tree node. \n",
    "        Try all features and thresholds. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_objects, n_features)\n",
    "            Objects in the parent node\n",
    "        y : ndarray, shape (n_objects, )\n",
    "            1D array with the object labels. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        best_split_column : int\n",
    "            Index of the best split column\n",
    "        best_threshold : float\n",
    "            The best split condition.\n",
    "        X_left : ndarray, shape (n_objects_l, n_features)\n",
    "            Objects in the left child\n",
    "        y_left : ndarray, shape (n_objects_l, )\n",
    "            Objects labels in the left child. \n",
    "        X_right : ndarray, shape (n_objects_r, n_features)\n",
    "            Objects in the right child\n",
    "        y_right : ndarray, shape (n_objects_r, )\n",
    "            Objects labels in the right child. \n",
    "        \"\"\"\n",
    "        \n",
    "        # To store best split parameters\n",
    "        best_split_column = None\n",
    "        best_threshold = None\n",
    "        # without splitting\n",
    "        best_cost = H(y) \n",
    "        \n",
    "        X_left = None\n",
    "        y_left = None\n",
    "        X_right = None\n",
    "        y_right = None\n",
    "        \n",
    "        #Далее код взят частично из семинара 7\n",
    "        \n",
    "        # Нужно минимизировать функцию Q, чтобы найти лучшее разбиение\n",
    "        for column_index in range(X.shape[1]):\n",
    "            #Выбираем все значения колонки\n",
    "            x_col = X[:, column_index]\n",
    "            for i_x in range(len(x_col)):\n",
    "                threshold = x_col[i_x]\n",
    "                #Если нашли разбиение лучше, чем было, то меняем параметры\n",
    "                Q_cur = Q(X, y, column_index, threshold)\n",
    "                if  Q_cur < best_cost:\n",
    "                    best_cost = Q_cur\n",
    "                    y_right = y[x_col > threshold]\n",
    "                    y_left = y[x_col <= threshold]\n",
    "                    X_right = X[x_col > threshold]\n",
    "                    X_left = X[x_col <= threshold]\n",
    "                    best_split_column = column_index\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        \n",
    "        return best_split_column, best_threshold, X_left, y_left, X_right, y_right\n",
    "    \n",
    "    def is_terminal(self, node, y):\n",
    "        \"\"\"\n",
    "        Check terminality conditions based on `max_depth` and `min_samples_split` parameters for a given node. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node, \n",
    "            \n",
    "        y : ndarray, shape (n_objects, )\n",
    "            Object labels. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Is_termial : bool\n",
    "            If True, node is terminal\n",
    "        \"\"\"\n",
    "        if node.depth >= self.max_depth:    \n",
    "            return True\n",
    "        if len(y) < self.min_samples_split:   \n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def grow_tree(self, node, X, y):\n",
    "        \"\"\"\n",
    "        Reccurently grow the tree from the `node` using a `X` and `y` as a dataset:\n",
    "         - check terminality conditions\n",
    "         - find best split if node is not terminal\n",
    "         - add child nodes to the node\n",
    "         - call the function recursively for the added child nodes\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node() object\n",
    "            Current node of the decision tree.\n",
    "        X : ndarray, shape (n_objects, n_features)\n",
    "            Objects \n",
    "        y : ndarray, shape (n_objects)\n",
    "            Labels\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.is_terminal(node, y):\n",
    "            node.is_terminal =True\n",
    "            return\n",
    "        if len(np.unique(y)) == 1:\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "        split_column, threshold, X_left, y_left, X_right, y_right = self.best_split(X, y)\n",
    "        \n",
    "        if split_column is None:\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "        \n",
    "        #Чтобы получилось совсем хорошо (как на семинаре, +- как встроенная реализация),\n",
    "        #я добавила параметр в конструктор min_samples_leaf\n",
    "        if len(X_left) < self.min_samples_leaf or len(X_right) < self.min_samples_leaf:\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "        \n",
    "        \n",
    "        node.column = split_column\n",
    "        node.threshold = threshold\n",
    "        \n",
    "        #так как ущел не terminal, то добавляем детей\n",
    "        node.left = Node()\n",
    "        node.left.depth = node.depth + 1\n",
    "        node.left.prediction = np.mean(y_left)\n",
    "        \n",
    "        node.right = Node()\n",
    "        node.right.depth = node.depth + 1\n",
    "        node.right.prediction = np.mean(y_right)\n",
    "        \n",
    "        self.grow_tree(node.right, X_right, y_right)\n",
    "        self.grow_tree(node.left, X_left, y_left)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Decision Tree Regressor.\n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The target values.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Check the input\n",
    "        if y is None:\n",
    "            raise ValueError('Y should not be None')\n",
    "        X, y = check_X_y(X, y, accept_sparse=False)\n",
    "        self.is_fitted_ = True\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        \n",
    "        # Initialize the tree (root node, depth = 0)\n",
    "        self.tree_ = Node()                             \n",
    "        self.tree_.depth = 0                            \n",
    "        self.tree_.prediction = np.mean(y)\n",
    "        \n",
    "        # Grow the tree\n",
    "        self.grow_tree(self.tree_, X, y)\n",
    "        return self        \n",
    "    \n",
    "    def get_prediction(self, node, x):\n",
    "        \"\"\"\n",
    "        Get prediction for an object `x`\n",
    "            - Return prediction of the `node` if it is terminal\n",
    "            - Otherwise, recursively call the function to get predictions of the proper child\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node() object\n",
    "            Current node of the decision tree.\n",
    "        x : ndarray, shape (n_features,)\n",
    "            Array of feature values of one object.\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : float\n",
    "            Prediction for an object x\n",
    "        \"\"\"\n",
    "        if node.is_terminal:    \n",
    "            return node.prediction\n",
    "        if x[node.column] > node.threshold:\n",
    "            y_pred = self.get_prediction(node.right, x)\n",
    "        else:\n",
    "            y_pred = self.get_prediction(node.left, x)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" \n",
    "        Get prediction for each object in X\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Returns predictions.\n",
    "        \"\"\"\n",
    "        # Check input and that `fit` had been called\n",
    "        X = check_array(X, accept_sparse=False)\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        \n",
    "        # Get predictions\n",
    "        y_predicted = []\n",
    "        for x in X:\n",
    "            y_curr = self.get_prediction(self.tree_, x)\n",
    "            y_predicted.append(y_curr)\n",
    "        return np.array(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check yourself\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "\n",
    "check_estimator(MyDecisionTreeRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 <a id=\"task4\"></a>  (0.5 points)\n",
    "\n",
    "Load boston dataset and split it on the train ($70\\%$) and test ($30\\%$). Fit Decision Tree of depth 1 and make the following plot:\n",
    "\n",
    "- Scatter plot of the traning points (selected for split feature on the x-axis, target variable on the y-axis)\n",
    "- Fitted model \n",
    "\n",
    "P.S. Depth of the tree is equal to the longest path from the root note to the leaf. Thus, tree with depth 1 will have exactly 1 split. \n",
    "\n",
    "P.P.S. Both fitted model and the training points should be on the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyDecisionTreeRegressor(max_depth=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_boston()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "DecisionTree = MyDecisionTreeRegressor(max_depth = 1)\n",
    "DecisionTree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Col 5, t 6.94, Pred: 23.02"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Видно, что если взять tree_, tree_.right/left и так далее, то можно узнать колонку разделения и порог\n",
    "DecisionTree.tree_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.939"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTree.tree_.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.083904109589042\n",
      "36.82419354838709\n"
     ]
    }
   ],
   "source": [
    "print(DecisionTree.tree_.left.prediction)\n",
    "print(DecisionTree.tree_.right.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = DecisionTree.tree_.column\n",
    "threshold = DecisionTree.tree_.threshold\n",
    "X_l = X_train[X_train[:, col] <= threshold]\n",
    "y_l = y_train[X_train[:, col] <= threshold]\n",
    "X_r = X_train[X_train[:,col] > threshold]\n",
    "y_r = y_train[X_train[:,col] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Train')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_l[:, col], y_l, color = 'orange')\n",
    "plt.scatter(X_r[:, col], y_r, color = 'pink')\n",
    "plt.axvline(threshold, color = 'salmon')\n",
    "plt.title('Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = DecisionTree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_l = X_test[X_test[:, col] <= threshold]\n",
    "y_l = y_test[X_test[:, col] <= threshold]\n",
    "X_r = X_test[X_test[:,col] > threshold]\n",
    "y_r = y_test[X_test[:,col] > threshold]\n",
    "\n",
    "y_l_pred = y_pred[X_test[:, col] <= threshold]\n",
    "y_r_pred = y_pred[X_test[:,col] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test and predicted')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_l[:, col], y_l, color = 'orange')\n",
    "plt.scatter(X_r[:, col], y_r, color = 'pink')\n",
    "\n",
    "plt.scatter(X_l[:, col], y_l_pred, color = 'brown')\n",
    "plt.scatter(X_r[:, col], y_r_pred, color = 'red')\n",
    "\n",
    "plt.axvline(threshold, color = 'salmon')\n",
    "plt.title('Test and predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из графиков, predict действительно похож на среднее по двум классам, разделенным пороговым значением. И, получается, что мы предсказываем значение y достаточно неплохо для такой маленькой глубины дерева."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 <a id=\"task5\"></a>  (0.5 points)\n",
    "\n",
    "Keep working with boston dataset. \n",
    "- Use `GridSearchCV` to find the best hyperparameters (`max_depth` and `min_samples_split`) on 5-Fold cross-validation\n",
    "- Train the model with the best set of hyperparameters on the whole train dataset. \n",
    "- Report `RMSE` on test dataset and hyperparameters of the best estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-8e0c177f5a10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mDecisionTree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyDecisionTreeRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDecisionTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'neg_root_mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Best parameters: {gs.best_params_}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1286\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1288\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    807\u001b[0m                                    (split_idx, (train, test)) in product(\n\u001b[0;32m    808\u001b[0m                                    \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m                                    enumerate(cv.split(X, y, groups))))\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'max_depth':range(1, 10), \n",
    "              'min_samples_split' : range(1, 15),\n",
    "              'min_samples_leaf':range(1, 5)}\n",
    "\n",
    "DecisionTree = MyDecisionTreeRegressor()\n",
    "gs = GridSearchCV(DecisionTree, parameters, scoring = 'neg_root_mean_squared_error', cv = 5, n_jobs = -1)\n",
    "gs.fit(X_train,y_train)\n",
    "\n",
    "print(f\"Best parameters: {gs.best_params_}\")\n",
    "print(f\"Best RMSE: {-1 * gs.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 <a id=\"task6\"></a>  (2 points)\n",
    "\n",
    "Recall definition of bias and variance:\n",
    "$$\n",
    "\\text{Bias}^2 = \\mathbb{E}_{p(x, y)} \\left[  (f(x) - \\mathbb{E}_{\\mathbb{X}}a_{\\mathbb{X}}(x))^2 \\right] \\\\\n",
    "\\text{Variance} = \\mathbb{E}_{p(x, y)} \\left[  \\mathbb{V}_{\\mathbb{X}}( a_{\\mathbb{X}}(x))  \\right]\n",
    "$$\n",
    "\n",
    "We wil now use use the following algorithm to estimate bias^2 and variance:\n",
    "\n",
    "1. Use bootsrap to create `n_iter` samples from the original dataset: $X_1, \\dots, X_{n\\_iter}$. Each $X_i$ has $N$ observations (randomly selected from the original dataset with replacement)\n",
    "2. For each bootstrapped sample define out-of-bag (OOB) sample $Z_1, \\dots, Z_{n\\_iter}$, which contains all the observations, which did not appear in the corresponding boostraped sample\n",
    "3. Fit the model on observations from $X_i$s and compute predictions on points from $Z_i$s\n",
    "4. For a given *object* $x_n$:\n",
    "     - bias^2: squared difference between true value $y_n$ and average prediction (average over the algorithms, for which $x_n$ was in OOB)\n",
    "     - variance: variance of the predictions (predictions of the algorithms, for which $x_n$ was in OOB)\n",
    "5. Average bias^2 and variance over all the points\n",
    "\n",
    "\n",
    "**Consider a toy example.** You are given a dataset with 5 observations: $((x_1 ,y_1), (x_2, y_2), (x_3, y_3), (x_4, y_4), (x_5, y_5))$, where $x_i$ is a vector of features, $y_i$ is a target variable . And choose `n_iter` to be 3. \n",
    "* Example of bootstrapped samples:\n",
    "$$X_1 = (x_2, x_5, x_2, x_3, x_2, x_5)$$\n",
    "$$X_2 = (x_5, x_2, x_4, x_4, x_1, x_5)$$\n",
    "$$X_3 = (x_1, x_3, x_1, x_4, x_3, x_1)$$\n",
    "\n",
    "* Corresponding OOB samples:\n",
    "$$Z_1 = (x_1, x_4)$$\n",
    "$$Z_2 = (x_3)$$\n",
    "$$Z_3 = (x_2, x_5)$$\n",
    "\n",
    "* Fit models using $X_1$, $X_2$ and $X_3$ as training data. Use 1st model to make predictions for points from $Z_1$, second - for $Z_2$, etc. and use these predictions to estimate bias and variance. \n",
    "\n",
    "    \n",
    "**Implement `get_bias_variance` function, using the algorithm above**\n",
    "\n",
    "*Note:*  You can only use 1 loop (for bootsrap iterations `n_iter`). All other operations should be vectorized. \n",
    "\n",
    "P.S. These numpy functions might be usefull here `np.nanmean`, `np.nanstd` (but you are not obliged to use them). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias_variance(estimator, x, y, n_iter):\n",
    "    \"\"\" \n",
    "    Calculate bias^2 and variance of the `estimator`. Using a given dataset and bootstrap with `n_iter` samples. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray, shape (n_samples, n_features)\n",
    "        The input samples.\n",
    "    y : ndarray, shape (n_samples, n_features)\n",
    "        The input samples.\n",
    "    n_iter: int\n",
    "        Number of samples in \n",
    "    Returns\n",
    "    -------\n",
    "    bias2 : float, \n",
    "        Estiamted squared bias\n",
    "    variance : float, \n",
    "        Estiamted variance\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    n = len(y)\n",
    "    lst_pred = np.repeat(np.nan, n) #первую строку затем нужно будет удалить (но иначе не получится concatenate, vstack)\n",
    "    for i in range(n_iter):\n",
    "        #1.\n",
    "        sample_indexes = np.random.choice(n, n, replace=True)\n",
    "        sample_x = x[sample_indexes,:]\n",
    "        sample_y = y[sample_indexes]\n",
    "        #2.\n",
    "        #Чтобы выбрать индексы, которые не попали в sample_index, нужно применить функцию isin и взять противоположные значения\n",
    "        #np.arange берем, чтобы взять только те индексы из всех, \n",
    "        #на месте которых в условии стоит True (то есть они не попали в выборку) \n",
    "        not_sample_indexes = np.arange(n)[~np.isin(range(n), sample_indexes)]\n",
    "        not_sample_x = x[not_sample_indexes,:]\n",
    "        not_sample_y = y[not_sample_indexes]\n",
    "        #3.\n",
    "        estimator.fit(sample_x,sample_y)\n",
    "        y_pred = estimator.predict(not_sample_x)\n",
    "\n",
    "        res_i = np.repeat(np.nan, n)\n",
    "        np.put(res_i, not_sample_indexes, y_pred)\n",
    "        lst_pred = np.hstack((lst_pred, res_i))\n",
    "        \n",
    "    #4.\n",
    "    #Нужно посчитать среднее по всем prediction, где индекс i попал в not_sample_indexes\n",
    "    #lst_pred = np.column_stack(lst_pred)\n",
    "    \n",
    "    #lst_pred = lst_pred[:, 1:]\n",
    "    lst_pred = lst_pred[1:]\n",
    "    #bias = np.square(y - np.nanmean(lst_pred, axis = 1))\n",
    "    bias = np.square(y - np.nanmean(lst_pred, axis = 0))\n",
    "    #5.\n",
    "    bias2 = np.nanmean(bias)\n",
    "    #variance = np.nanmean(np.nanvar(lst_pred, axis = 1))\n",
    "    variance = np.nanmean(np.nanvar(lst_pred, axis = 0))\n",
    "    return bias2, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87.95791984100343, 86.9508968762377)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "estimator = MyDecisionTreeRegressor(max_depth=8, min_samples_split=15)\n",
    "\n",
    "get_bias_variance(estimator, X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 <a id=\"task7\"></a>  (0.5 points = 0.25 for code + 0.25 for comments)\n",
    "\n",
    "Compute bias and variance for the trees of different depths. Plot how bias and variance change as depth increases. \n",
    "\n",
    "Comment on what you observe, how does your result correspond to what we have discussed in class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_lst = []\n",
    "variance_lst = []\n",
    "for depth in range(1, 15):\n",
    "    estimator = MyDecisionTreeRegressor(max_depth=depth, min_samples_split=14)\n",
    "    bias, variance = get_bias_variance(estimator, X_train, y_train, 10)\n",
    "    bias_lst.append(bias)\n",
    "    variance_lst.append(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, 15), bias_lst, color = 'orange', label = 'Bias')\n",
    "plt.plot(range(1, 15), variance_lst, color = 'salmon', label = 'Variance')\n",
    "plt.xlabel('Depth of tree')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Мы знаем, что дисперсия отвечает по сути за переобучение. Поэтому закономерно, что при большой глубине дерева дисперсия начинает возрастать. Значит, происходит переобучение модели. Смещение (bias) напротив, отвечает за недообучение. Поэтому закономерно, что оно падает в зависимости о глубины (или почти не меняется на больших значениях) ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 <a id=\"task8\"></a>  (0.5 points = 0.25 for code + 0.25 for comments)\n",
    "\n",
    "Let's try to reduce variance with bagging. Use `sklearn.ensemble.BaggingRegressor` to get an ensemble and compute its bias and variance. \n",
    "\n",
    "Answer the following questions:\n",
    " - How bagging should affect bias and variance in theory?\n",
    " - How bias and variance change (if they change) compared to an individual tree in you experiments? \n",
    " - Do your results align with the theory? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16.802448534553175, 1.7510161032150733)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "#возьмем параметры модели из Task 6 для сравнения результатов\n",
    "model = BaggingRegressor(base_estimator=MyDecisionTreeRegressor(max_depth=8, min_samples_split=15),\n",
    "                         n_jobs=-1, n_estimators=100, random_state = 42)\n",
    "get_bias_variance(model, X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Как видно из результатов, bias и variance довольно сильно уменьшились. В теории, при баггинге должна уменьшаться variance, но если вдруг до этого variance параметр уже был оптимальный и лучший, то bias может и увеличиться. В нашем случае, оба параметра уменьшились, что говорит о том, что они не были оптимальными в предыдущем случае (individual tree). Получается, что результат согласуется с теорией.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. More Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will be working with [Thyroid Disease Data Set](https://archive.ics.uci.edu/ml/datasets/thyroid+disease) to solve a classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>on_thyroxine</th>\n",
       "      <th>query_on_thyroxine</th>\n",
       "      <th>on_antithyroid_medication</th>\n",
       "      <th>sick</th>\n",
       "      <th>pregnant</th>\n",
       "      <th>thyroid_surgery</th>\n",
       "      <th>I131_treatment</th>\n",
       "      <th>query_hypothyroid</th>\n",
       "      <th>...</th>\n",
       "      <th>T3</th>\n",
       "      <th>TT4_measured</th>\n",
       "      <th>TT4</th>\n",
       "      <th>T4U_measured</th>\n",
       "      <th>T4U</th>\n",
       "      <th>FTI_measured</th>\n",
       "      <th>FTI</th>\n",
       "      <th>TBG_measured</th>\n",
       "      <th>TBG</th>\n",
       "      <th>referral_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>t</td>\n",
       "      <td>125.0</td>\n",
       "      <td>t</td>\n",
       "      <td>1.14</td>\n",
       "      <td>t</td>\n",
       "      <td>109.0</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SVHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.0</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>t</td>\n",
       "      <td>102.0</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.0</td>\n",
       "      <td>M</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>109.0</td>\n",
       "      <td>t</td>\n",
       "      <td>0.91</td>\n",
       "      <td>t</td>\n",
       "      <td>120.0</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70.0</td>\n",
       "      <td>F</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>t</td>\n",
       "      <td>175.0</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70.0</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>t</td>\n",
       "      <td>61.0</td>\n",
       "      <td>t</td>\n",
       "      <td>0.87</td>\n",
       "      <td>t</td>\n",
       "      <td>70.0</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SVI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age sex on_thyroxine query_on_thyroxine on_antithyroid_medication sick  \\\n",
       "0  41.0   F            f                  f                         f    f   \n",
       "1  23.0   F            f                  f                         f    f   \n",
       "2  46.0   M            f                  f                         f    f   \n",
       "3  70.0   F            t                  f                         f    f   \n",
       "4  70.0   F            f                  f                         f    f   \n",
       "\n",
       "  pregnant thyroid_surgery I131_treatment query_hypothyroid  ...   T3  \\\n",
       "0        f               f              f                 f  ...  2.5   \n",
       "1        f               f              f                 f  ...  2.0   \n",
       "2        f               f              f                 f  ...  NaN   \n",
       "3        f               f              f                 f  ...  1.9   \n",
       "4        f               f              f                 f  ...  1.2   \n",
       "\n",
       "  TT4_measured    TT4 T4U_measured   T4U FTI_measured    FTI  TBG_measured  \\\n",
       "0            t  125.0            t  1.14            t  109.0             f   \n",
       "1            t  102.0            f   NaN            f    NaN             f   \n",
       "2            t  109.0            t  0.91            t  120.0             f   \n",
       "3            t  175.0            f   NaN            f    NaN             f   \n",
       "4            t   61.0            t  0.87            t   70.0             f   \n",
       "\n",
       "  TBG  referral_source  \n",
       "0 NaN             SVHC  \n",
       "1 NaN            other  \n",
       "2 NaN            other  \n",
       "3 NaN            other  \n",
       "4 NaN              SVI  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('thyroid_disease.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Class'])\n",
    "X = df.drop('Class', axis=1)\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 <a id=\"task2_1\"></a> (1 point)\n",
    "\n",
    "Let's start with data preprocessing. \n",
    "\n",
    "0. Drop columns, which are not usefull (e.g. a lot of missing values). Motivate your choice. \n",
    "1. Split dataset into train and test\n",
    "2. You've probably noticed that we have both categorical and numerical columns. Here is what you need to do with them:\n",
    "    - Categorical: Fill missing values and apply one-hot-encoding (take a look at the argument `drop` in [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to figure out the best way to apply OHE to binary variables)\n",
    "    - Numeric: Fill missing values\n",
    "    \n",
    "Use `ColumnTranformer` ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer)) to define a single transformer for all the columns in the dataset. It takes as input a list of tuples\n",
    "\n",
    "```\n",
    "ColumnTransformer([\n",
    "    ('name1', transorm1, column_names1),\n",
    "    ('name2', transorm2, column_names2)\n",
    "])\n",
    "```\n",
    "\n",
    "Pay attention to an argument `remainder='passthrough'`. [Here](https://scikit-learn.org/stable/modules/compose.html#column-transformer) you can find some examples of how to use column transformer. \n",
    "    \n",
    "Since we want to apply 2 transformations to categorical feature, it is very convenient to combine them into a `Pipeline`:\n",
    "\n",
    "```\n",
    "double_transform = make_pipeline(\n",
    "                                 transform_1,\n",
    "                                 transform_2\n",
    "                                )\n",
    "```\n",
    "\n",
    "\n",
    "P.S. Choose your favourite way to fill missing values. \n",
    "\n",
    "*Hint* Categorical column usually have `dtype = 'object'`. This may help to obtain list of categorical and numerical columns in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#0. Dropping columns with missing values\n",
    "#Выражение X.isna().mean() показывает по сути процент nan в колонках. Нам нужно удалить те колонки, где NaN много.\n",
    "#Возьмем в качестве порога 0.1 (так как 10% потери данных - это уже достаточно много)\n",
    "X = X.drop(columns = (X.columns[X.isna().mean() > 0.1]).tolist())\n",
    "\n",
    "#1. Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "\n",
    "#2. Fillilng missing values and applying one-hot-encoding\n",
    "cat_columns = X.columns[X.dtypes == 'object']\n",
    "num_columns = X.columns[X.dtypes != 'object']\n",
    "print(len(cat_columns) + len(num_columns)) #видим, что все колонки распределились на categorical и numerical\n",
    "\n",
    "\n",
    "# define column_transformer \n",
    "transform1 = OneHotEncoder()\n",
    "transform2 = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "categorical_transfrom = make_pipeline(transform1, transform2)\n",
    "numerical_transfrom = SimpleImputer(strategy='mean')\n",
    "\n",
    "column_transformer = ColumnTransformer(transformers= [('numerical', numerical_transfrom, num_columns),\n",
    "                                                      ('categorical', categorical_transfrom, cat_columns)],\n",
    "                                       remainder='passthrough')\n",
    "# Transform the data\n",
    "X_train = column_transformer.fit_transform(X_train)\n",
    "X_test = column_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 <a id=\"task2_2\"></a> (0.7 points  = 0.4 for code + 0.3 for comments)\n",
    "\n",
    "Fit and compare 5 different models (use sklearn): Gradient Boosting, Random Forest, Decision Tree, SVM, Logitics Regression\n",
    "    \n",
    "* Choose one classification metric and justify your choice .\n",
    "* Compare the models using score on cross validation. Mind the class balance when choosing the cross validation. (You can read more about different CV strategies [here](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold))\n",
    "* Which model has the best performance? Which models overfit or underfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting\n",
      "Score on Cross-Validation 0.9295\n",
      "Actual test score: 0.9488\n",
      "Random Forest\n",
      "Score on Cross-Validation 0.9330\n",
      "Actual test score: 0.9488\n",
      "Decision Tree\n",
      "Score on Cross-Validation 0.9008\n",
      "Actual test score: 0.9019\n",
      "SVC\n",
      "Score on Cross-Validation 0.9356\n",
      "Actual test score: 0.9461\n",
      "Logistic Regression\n",
      "Score on Cross-Validation 0.9360\n",
      "Actual test score: 0.9470\n"
     ]
    }
   ],
   "source": [
    "#Определяем все 5 моделей\n",
    "models = [\n",
    "    GradientBoostingClassifier(), RandomForestClassifier(),\n",
    "    DecisionTreeClassifier(), SVC(), LogisticRegression()]\n",
    "names = ['Gradient Boosting', 'Random Forest','Decision Tree','SVC','Logistic Regression']\n",
    "cv = StratifiedKFold() # 5 splits - defeault\n",
    "\n",
    "for i in range(len(models)):\n",
    "    models[i].fit(X_train, y_train)\n",
    "    #Частично код с семинара\n",
    "    print(names[i])\n",
    "    print(f'Score on Cross-Validation {cross_val_score(models[i], X_train, y_train, scoring=\"accuracy\").mean():.4f}')\n",
    "    print(f'Actual test score: {accuracy_score(models[i].predict(X_test), y_test):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Получается, что лучший accuracy score получается при использовании модели Random Forest и Gradient Boosting. Кажется, что Decision Tree модель могла переобучиться, потому что результат ощутимо меньше, чем для других моделей, и кроме того, decision tree в целом быстро переобучается. Остальные модели идут почти ровно. Вряд ли можно говорить о недообучении.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 <a id=\"task2_3\"></a> (0.5 points = 0.4 for code + 0.1 for comments)\n",
    "\n",
    "More Gradient Boosting. Choose one of the three popular boosting implementations (xgboost, lightgbm, catboost). Select hyperparameters (number of trees, learning rate, depth) on cross-validation and compare with the methods from the previous task. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Я брала код частично из источника https://machinelearningmastery.com/extreme-gradient-boosting-ensemble-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.2.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.16.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:50:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'max_depth':range(1, 3), \n",
    "              'learning_rate' : [0.01, 0.1]}\n",
    "\n",
    "model = XGBClassifier()\n",
    "gs = GridSearchCV(model, parameters, scoring = 'neg_root_mean_squared_error', cv = 3, n_jobs = -1)\n",
    "gs.fit(X_train,y_train)\n",
    "\n",
    "print(f\"Best parameters: {gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:50:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n",
       "              max_depth=1, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=4,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBClassifier(max_depth = 1, learning_rate = 0.01)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:50:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:50:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:50:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:50:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:50:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9356\n",
      "test score: 0.9461\n"
     ]
    }
   ],
   "source": [
    "print(f'{cross_val_score(model, X_train, y_train, scoring=\"accuracy\").mean():.4f}')\n",
    "print(f'test score: {accuracy_score(model.predict(X_test), y_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```На тесте accuracy оказалось примерно таким же, как и в предыдущих моделях. В принципе, качество хорошее и значимых отличий от предыдущих моделей я не вижу.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 <a id=\"task2_4\"></a> (0.7 points = 0.4 for code + 0.3 for comments)\n",
    "\n",
    "Now let's train more fancy ensembles:\n",
    "\n",
    "* Bagging with decision trees as base estimators\n",
    "* Bagging with gradient boosting as base estimators (use large amount of trees in gradient boosting, >100)\n",
    "* [Voting classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) \n",
    "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Logistic Regression as a final model\n",
    "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Gradeint Boosting as a final model\n",
    "\n",
    "\n",
    "If not stated in the task, feel free to tune / choose hyperparameters and base models.\n",
    "\n",
    "Answer the questions:\n",
    "* Which model has the best performance?\n",
    "* Does bagging reduce overfitting of the gradient boosting with large amount of trees? \n",
    "* What is the difference between voting and staking? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged Decision Tree with max_depth = 2: 0.9461\n",
      "Bagged Decision Tree with max_depth = 3: 0.9461\n",
      "Bagged Decision Tree with max_depth = 5: 0.9488\n",
      "Bagged Decision Tree with max_depth = 7: 0.9470\n"
     ]
    }
   ],
   "source": [
    "for max_depth in [2, 3, 5, 7]:\n",
    "    BaggingDecisionTree = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=max_depth), \n",
    "                                            n_jobs=-1, n_estimators=100)\n",
    "    BaggingDecisionTree.fit(X_train, y_train)\n",
    "    print(f'Bagged Decision Tree with max_depth = {max_depth}: {accuracy_score(BaggingDecisionTree.predict(X_test), y_test):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged Gradient Boosting with max_depth = 2: 0.9496\n",
      "Bagged Gradient Boosting with max_depth = 3: 0.9514\n",
      "Bagged Gradient Boosting with max_depth = 5: 0.9523\n",
      "Bagged Gradient Boosting with max_depth = 7: 0.9523\n"
     ]
    }
   ],
   "source": [
    "for max_depth in [2, 3, 5, 7]:\n",
    "    BaggingGradientBoosting = BaggingClassifier(base_estimator=GradientBoostingClassifier(max_depth=max_depth),\n",
    "                                            n_jobs=-1, n_estimators=200)\n",
    "    BaggingGradientBoosting.fit(X_train, y_train)\n",
    "    print(f'Bagged Gradient Boosting with max_depth = {max_depth}: {accuracy_score(BaggingGradientBoosting.predict(X_test), y_test):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClissifier with voting = hard: 0.9496\n",
      "VotingClissifier with voting = soft: 0.9267\n"
     ]
    }
   ],
   "source": [
    "#код частично из документации по Voiting\n",
    "clf1 = LogisticRegression(random_state=42)\n",
    "clf2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf3 = GaussianNB()\n",
    "for voting in ['hard', 'soft']:\n",
    "    Voting = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting = voting, n_jobs = -1)\n",
    "    Voting.fit(X_train, y_train)\n",
    "    print(f'VotingClissifier with voting = {voting}: {accuracy_score(Voting.predict(X_test), y_test):.4f}')                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Logistic Regression: 0.9488\n"
     ]
    }
   ],
   "source": [
    "#Код взят частично из документации https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier\n",
    "estimators = [('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "              ('svr', make_pipeline(StandardScaler(),LinearSVC(random_state=42)))]\n",
    "StackingLogisticRegression = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), n_jobs = -1)\n",
    "StackingLogisticRegression.fit(X_train, y_train)\n",
    "print(f'Stacking Logistic Regression: {accuracy_score(StackingLogisticRegression.predict(X_test), y_test):.4f}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Gradient Boosting with max_depth = 1: 0.9496\n",
      "Stacking Gradient Boosting with max_depth = 2: 0.9532\n",
      "Stacking Gradient Boosting with max_depth = 3: 0.9470\n",
      "Stacking Gradient Boosting with max_depth = 5: 0.9355\n",
      "Stacking Gradient Boosting with max_depth = 7: 0.9293\n"
     ]
    }
   ],
   "source": [
    "for max_depth in [1, 2, 3, 5, 7]:\n",
    "    StackingGradientBoosting = StackingClassifier(estimators=estimators,\n",
    "                                                  final_estimator=GradientBoostingClassifier(max_depth = max_depth),\n",
    "                                                  n_jobs = -1)\n",
    "    StackingGradientBoosting.fit(X_train, y_train)\n",
    "    print(f'Stacking Gradient Boosting with max_depth = {max_depth}: {accuracy_score(StackingGradientBoosting.predict(X_test), y_test):.4f}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Результаты сопоставимы. Но все-таки лучший результат по accuracy score получился в модели Stacking Classifier with Gradeint Boosting as a final model с максимальной глубиной = 2. ```\n",
    "``` Баггинг действительно помогает gradient boosting, score увеличился на ~0.03. То есть переобучение меньше при большом количестве деревьев.```\n",
    "```Stacking и Voiting различается тем, что Stacking использует final model как слой, обхединяющий все estimators. Voiting немного менее умный и использует их по отдельности. Значит, результат у Stacking должен быть чуть больше, как в принципе и получается в случае voting = soft, а в случае voting = hard результат не сильно отличается. ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 <a id=\"task2_5\"></a> (0.1 points)\n",
    "\n",
    "Report the test score for the best model, that you were able to get on this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.9505\n"
     ]
    }
   ],
   "source": [
    "StackingGradientBoosting = StackingClassifier(estimators=estimators,\n",
    "                                                  final_estimator=GradientBoostingClassifier(max_depth = 2),\n",
    "                                                  n_jobs = -1)\n",
    "StackingGradientBoosting.fit(X_train, y_train)\n",
    "print(f'Test score: {accuracy_score(StackingGradientBoosting.predict(X_test), y_test):.4f}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Получается, что 0.95 довольно высокая точность, которой удалось добиться.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
